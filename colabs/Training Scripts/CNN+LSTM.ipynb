{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "753f1710-04a3-487b-93f7-ca580d8e8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f419d7cb-e814-4f20-af4f-e8cb3ae4a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths\n",
    "folders = {\n",
    "    \"Abnormal Heartbeat\": r\"D:\\FYP\\Cadivas CNN\\preprocessed_1d\\AHB\",\n",
    "    \"Myocardial Infarction\": r\"D:\\FYP\\Cadivas CNN\\preprocessed_1d\\MI\",\n",
    "    \"Normal\": r\"D:\\FYP\\Cadivas CNN\\preprocessed_1d\\NORMAL\",\n",
    "    \"History of MI\": r\"D:\\FYP\\Cadivas CNN\\preprocessed_1d\\PM\"\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "for label, folder in folders.items():\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(folder, file))\n",
    "            df['Class'] = label\n",
    "            all_data.append(df)\n",
    "\n",
    "# Combine all data\n",
    "data = pd.concat(all_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3a1fb5e-4cf4-4312-a12e-f92a33d60e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (first 255 columns)\n",
    "X = data.iloc[:, :255].values  \n",
    "y = data['Class'].values\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d76864ac-21ac-47ec-a606-a9641156294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_onehot, test_size=0.2, random_state=42, stratify=y_onehot\n",
    ")\n",
    "\n",
    "# Reshape for CNN+LSTM (samples, timesteps, features=1)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e011433f-8817-4ba0-b461-1c65bdabbc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 0.9957081545064378, 1: 1.3488372093023255, 2: 0.9707112970711297, 3: 0.8169014084507042}\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_encoded),\n",
    "    y=y_encoded\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f9664b4-f478-47e5-9c1b-eec6e6870cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_5 (Conv1D)           (None, 255, 64)           384       \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 255, 64)          256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 127, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 127, 128)          41088     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 127, 128)         512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_6 (MaxPooling  (None, 63, 128)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               16640     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,316\n",
      "Trainable params: 108,932\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], 1)),\n",
    "\n",
    "    Conv1D(64, kernel_size=5, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Conv1D(128, kernel_size=5, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    LSTM(64, return_sequences=False),\n",
    "\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y_onehot.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c51fbe4d-802c-4ff7-9244-e0a0f444edf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dedfb79b-ef4b-454f-b559-27dbe2c2c2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "151/151 [==============================] - 13s 68ms/step - loss: 1.3429 - accuracy: 0.3421 - val_loss: 1.4125 - val_accuracy: 0.2706 - lr: 0.0010\n",
      "Epoch 2/120\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 1.2424 - accuracy: 0.4338 - val_loss: 1.3571 - val_accuracy: 0.3336 - lr: 0.0010\n",
      "Epoch 3/120\n",
      "151/151 [==============================] - 10s 64ms/step - loss: 1.1258 - accuracy: 0.5109 - val_loss: 1.2229 - val_accuracy: 0.4588 - lr: 0.0010\n",
      "Epoch 4/120\n",
      "151/151 [==============================] - 10s 65ms/step - loss: 1.0019 - accuracy: 0.5808 - val_loss: 1.0229 - val_accuracy: 0.5640 - lr: 0.0010\n",
      "Epoch 5/120\n",
      "151/151 [==============================] - 10s 65ms/step - loss: 0.9130 - accuracy: 0.6279 - val_loss: 0.8825 - val_accuracy: 0.6386 - lr: 0.0010\n",
      "Epoch 6/120\n",
      "151/151 [==============================] - 10s 64ms/step - loss: 0.8225 - accuracy: 0.6726 - val_loss: 0.8274 - val_accuracy: 0.6755 - lr: 0.0010\n",
      "Epoch 7/120\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.7620 - accuracy: 0.6903 - val_loss: 0.7910 - val_accuracy: 0.6635 - lr: 0.0010\n",
      "Epoch 8/120\n",
      "151/151 [==============================] - 10s 67ms/step - loss: 0.6930 - accuracy: 0.7252 - val_loss: 0.6743 - val_accuracy: 0.7290 - lr: 0.0010\n",
      "Epoch 9/120\n",
      "151/151 [==============================] - 10s 64ms/step - loss: 0.6296 - accuracy: 0.7431 - val_loss: 0.7029 - val_accuracy: 0.7004 - lr: 0.0010\n",
      "Epoch 10/120\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.6050 - accuracy: 0.7586 - val_loss: 0.6013 - val_accuracy: 0.7567 - lr: 0.0010\n",
      "Epoch 11/120\n",
      "151/151 [==============================] - 10s 65ms/step - loss: 0.5488 - accuracy: 0.7817 - val_loss: 0.6377 - val_accuracy: 0.7414 - lr: 0.0010\n",
      "Epoch 12/120\n",
      "151/151 [==============================] - 10s 65ms/step - loss: 0.5194 - accuracy: 0.7959 - val_loss: 0.6302 - val_accuracy: 0.7451 - lr: 0.0010\n",
      "Epoch 13/120\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.4762 - accuracy: 0.8154 - val_loss: 0.5861 - val_accuracy: 0.7663 - lr: 0.0010\n",
      "Epoch 14/120\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.4528 - accuracy: 0.8237 - val_loss: 0.5069 - val_accuracy: 0.7953 - lr: 0.0010\n",
      "Epoch 15/120\n",
      "151/151 [==============================] - 10s 65ms/step - loss: 0.4352 - accuracy: 0.8305 - val_loss: 0.5406 - val_accuracy: 0.7795 - lr: 0.0010\n",
      "Epoch 16/120\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.4148 - accuracy: 0.8389 - val_loss: 0.5108 - val_accuracy: 0.8044 - lr: 0.0010\n",
      "Epoch 17/120\n",
      "151/151 [==============================] - 10s 65ms/step - loss: 0.3627 - accuracy: 0.8580 - val_loss: 0.4976 - val_accuracy: 0.8106 - lr: 0.0010\n",
      "Epoch 18/120\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.3666 - accuracy: 0.8580 - val_loss: 0.5005 - val_accuracy: 0.8210 - lr: 0.0010\n",
      "Epoch 19/120\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.3181 - accuracy: 0.8765 - val_loss: 0.4702 - val_accuracy: 0.8297 - lr: 0.0010\n",
      "Epoch 20/120\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.2945 - accuracy: 0.8892 - val_loss: 0.4411 - val_accuracy: 0.8305 - lr: 0.0010\n",
      "Epoch 21/120\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.2829 - accuracy: 0.8903 - val_loss: 0.4336 - val_accuracy: 0.8363 - lr: 0.0010\n",
      "Epoch 22/120\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.2676 - accuracy: 0.8995 - val_loss: 0.4563 - val_accuracy: 0.8342 - lr: 0.0010\n",
      "Epoch 23/120\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.2497 - accuracy: 0.9049 - val_loss: 0.5142 - val_accuracy: 0.8189 - lr: 0.0010\n",
      "Epoch 24/120\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.2525 - accuracy: 0.9021 - val_loss: 0.4292 - val_accuracy: 0.8537 - lr: 0.0010\n",
      "Epoch 25/120\n",
      "151/151 [==============================] - 10s 67ms/step - loss: 0.2371 - accuracy: 0.9105 - val_loss: 0.4819 - val_accuracy: 0.8471 - lr: 0.0010\n",
      "Epoch 26/120\n",
      "151/151 [==============================] - 10s 69ms/step - loss: 0.2204 - accuracy: 0.9199 - val_loss: 0.3768 - val_accuracy: 0.8827 - lr: 0.0010\n",
      "Epoch 27/120\n",
      "151/151 [==============================] - 10s 67ms/step - loss: 0.2048 - accuracy: 0.9220 - val_loss: 0.4320 - val_accuracy: 0.8591 - lr: 0.0010\n",
      "Epoch 28/120\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.1919 - accuracy: 0.9299 - val_loss: 0.4013 - val_accuracy: 0.8637 - lr: 0.0010\n",
      "Epoch 29/120\n",
      "151/151 [==============================] - 10s 67ms/step - loss: 0.1715 - accuracy: 0.9381 - val_loss: 0.4515 - val_accuracy: 0.8521 - lr: 0.0010\n",
      "Epoch 30/120\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.1790 - accuracy: 0.9323 - val_loss: 0.3989 - val_accuracy: 0.8827 - lr: 0.0010\n",
      "Epoch 31/120\n",
      "151/151 [==============================] - ETA: 0s - loss: 0.1488 - accuracy: 0.9439\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "151/151 [==============================] - 10s 67ms/step - loss: 0.1488 - accuracy: 0.9439 - val_loss: 0.3833 - val_accuracy: 0.8848 - lr: 0.0010\n",
      "Epoch 32/120\n",
      "151/151 [==============================] - 10s 67ms/step - loss: 0.1034 - accuracy: 0.9627 - val_loss: 0.3508 - val_accuracy: 0.8989 - lr: 5.0000e-04\n",
      "Epoch 33/120\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.0847 - accuracy: 0.9702 - val_loss: 0.3780 - val_accuracy: 0.9097 - lr: 5.0000e-04\n",
      "Epoch 34/120\n",
      "151/151 [==============================] - 10s 67ms/step - loss: 0.0839 - accuracy: 0.9702 - val_loss: 0.3543 - val_accuracy: 0.9121 - lr: 5.0000e-04\n",
      "Epoch 35/120\n",
      "151/151 [==============================] - 10s 68ms/step - loss: 0.0810 - accuracy: 0.9717 - val_loss: 0.4029 - val_accuracy: 0.9047 - lr: 5.0000e-04\n",
      "Epoch 36/120\n",
      "151/151 [==============================] - 10s 67ms/step - loss: 0.0807 - accuracy: 0.9687 - val_loss: 0.4083 - val_accuracy: 0.9022 - lr: 5.0000e-04\n",
      "Epoch 37/120\n",
      "151/151 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9753\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.0701 - accuracy: 0.9753 - val_loss: 0.3816 - val_accuracy: 0.9113 - lr: 5.0000e-04\n",
      "Epoch 38/120\n",
      "151/151 [==============================] - 10s 68ms/step - loss: 0.0565 - accuracy: 0.9809 - val_loss: 0.3848 - val_accuracy: 0.9167 - lr: 2.5000e-04\n",
      "Epoch 39/120\n",
      "151/151 [==============================] - 10s 67ms/step - loss: 0.0483 - accuracy: 0.9837 - val_loss: 0.3604 - val_accuracy: 0.9175 - lr: 2.5000e-04\n",
      "Epoch 40/120\n",
      "151/151 [==============================] - 10s 67ms/step - loss: 0.0479 - accuracy: 0.9837 - val_loss: 0.3503 - val_accuracy: 0.9171 - lr: 2.5000e-04\n",
      "Epoch 41/120\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.0452 - accuracy: 0.9854 - val_loss: 0.3526 - val_accuracy: 0.9287 - lr: 2.5000e-04\n",
      "Epoch 42/120\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.0441 - accuracy: 0.9850 - val_loss: 0.3755 - val_accuracy: 0.9250 - lr: 2.5000e-04\n",
      "Epoch 43/120\n",
      "151/151 [==============================] - 10s 67ms/step - loss: 0.0488 - accuracy: 0.9837 - val_loss: 0.3704 - val_accuracy: 0.9213 - lr: 2.5000e-04\n",
      "Epoch 44/120\n",
      "151/151 [==============================] - 10s 67ms/step - loss: 0.0413 - accuracy: 0.9869 - val_loss: 0.3697 - val_accuracy: 0.9225 - lr: 2.5000e-04\n",
      "Epoch 45/120\n",
      "151/151 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9880\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "151/151 [==============================] - 10s 66ms/step - loss: 0.0399 - accuracy: 0.9880 - val_loss: 0.3812 - val_accuracy: 0.9275 - lr: 2.5000e-04\n",
      "Epoch 46/120\n",
      "151/151 [==============================] - 10s 67ms/step - loss: 0.0360 - accuracy: 0.9889 - val_loss: 0.3879 - val_accuracy: 0.9271 - lr: 1.2500e-04\n",
      "Epoch 47/120\n",
      "151/151 [==============================] - 10s 68ms/step - loss: 0.0322 - accuracy: 0.9908 - val_loss: 0.3895 - val_accuracy: 0.9295 - lr: 1.2500e-04\n",
      "Epoch 48/120\n",
      "151/151 [==============================] - 10s 67ms/step - loss: 0.0292 - accuracy: 0.9926 - val_loss: 0.3859 - val_accuracy: 0.9271 - lr: 1.2500e-04\n",
      "Epoch 49/120\n",
      "151/151 [==============================] - 10s 69ms/step - loss: 0.0287 - accuracy: 0.9916 - val_loss: 0.4144 - val_accuracy: 0.9262 - lr: 1.2500e-04\n",
      "Epoch 50/120\n",
      "151/151 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9933\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "151/151 [==============================] - 10s 67ms/step - loss: 0.0266 - accuracy: 0.9933 - val_loss: 0.4001 - val_accuracy: 0.9295 - lr: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=120,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[lr_scheduler, early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf65c7e6-8449-4a2a-a87e-1c02691725a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 1s 10ms/step\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "   Abnormal Heartbeat       0.94      0.83      0.88       606\n",
      "        History of MI       0.80      0.90      0.85       447\n",
      "Myocardial Infarction       0.97      1.00      0.99       621\n",
      "               Normal       0.93      0.93      0.93       739\n",
      "\n",
      "             accuracy                           0.92      2413\n",
      "            macro avg       0.91      0.91      0.91      2413\n",
      "         weighted avg       0.92      0.92      0.92      2413\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[504  64  10  28]\n",
      " [ 17 402   2  26]\n",
      " [  0   0 621   0]\n",
      " [ 14  35   4 686]]\n",
      "\n",
      "Final Test Accuracy: 91.71%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "y_pred_labels = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test_labels, y_pred_labels, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_test_labels, y_pred_labels)\n",
    "print(\"\\nFinal Test Accuracy: {:.2f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4c7b3ae-d52c-4ce3-9345-9d4f945ba536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoder(cnn+lstm).pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(\"CNN+LSTM(91).h5\")\n",
    "import joblib\n",
    "joblib.dump(label_encoder, \"label_encoder(cnn+lstm).pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12197d33-0642-4e52-883a-8ab9494c1180",
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34e1504f-cde5-4882-9706-b2cd761901df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Bidirectional, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4628f3b0-a47c-402e-8873-db8695a937b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume X, y already prepared (X: signals, y: labels)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "y_onehot = tf.keras.utils.to_categorical(y_encoded)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_onehot, test_size=0.2, random_state=42, stratify=y_onehot\n",
    ")\n",
    "\n",
    "# Expand dims for Conv1D (samples, timesteps, channels)\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "\n",
    "# Augmentation: add Gaussian noise\n",
    "def add_noise(X, noise_factor=0.01):\n",
    "    return X + noise_factor * np.random.normal(size=X.shape)\n",
    "\n",
    "X_train_noisy = add_noise(X_train)\n",
    "y_train_noisy = y_train.copy()\n",
    "\n",
    "# Concatenate original + augmented\n",
    "X_train_aug = np.concatenate([X_train, X_train_noisy])\n",
    "y_train_aug = np.concatenate([y_train, y_train_noisy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ba85a5d-b9cb-46d4-97f0-4cc8b8e43c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], 1)),\n",
    "\n",
    "    # CNN Feature Extractor\n",
    "    Conv1D(128, kernel_size=5, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Conv1D(256, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # BiLSTM for temporal dependencies\n",
    "    Bidirectional(LSTM(128, return_sequences=False)),\n",
    "\n",
    "    # Fully connected\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y_onehot.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b48f2d69-373e-45a6-8977-9078bc9808db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "302/302 [==============================] - 220s 715ms/step - loss: 1.2327 - accuracy: 0.4370 - val_loss: 1.5834 - val_accuracy: 0.3050 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "302/302 [==============================] - 193s 639ms/step - loss: 0.9508 - accuracy: 0.6076 - val_loss: 1.1362 - val_accuracy: 0.5234 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "302/302 [==============================] - 190s 630ms/step - loss: 0.7041 - accuracy: 0.7142 - val_loss: 0.6411 - val_accuracy: 0.7402 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "302/302 [==============================] - 194s 642ms/step - loss: 0.5378 - accuracy: 0.7860 - val_loss: 0.5753 - val_accuracy: 0.7700 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "302/302 [==============================] - 188s 622ms/step - loss: 0.4174 - accuracy: 0.8367 - val_loss: 0.4578 - val_accuracy: 0.8239 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "302/302 [==============================] - 191s 631ms/step - loss: 0.3378 - accuracy: 0.8702 - val_loss: 0.4471 - val_accuracy: 0.8388 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "302/302 [==============================] - 190s 629ms/step - loss: 0.2552 - accuracy: 0.9035 - val_loss: 0.4220 - val_accuracy: 0.8525 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "302/302 [==============================] - 189s 627ms/step - loss: 0.2080 - accuracy: 0.9228 - val_loss: 0.4171 - val_accuracy: 0.8628 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "302/302 [==============================] - 190s 628ms/step - loss: 0.1605 - accuracy: 0.9419 - val_loss: 0.3683 - val_accuracy: 0.8893 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "302/302 [==============================] - 191s 633ms/step - loss: 0.1302 - accuracy: 0.9541 - val_loss: 0.4043 - val_accuracy: 0.8757 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "302/302 [==============================] - 194s 641ms/step - loss: 0.1198 - accuracy: 0.9579 - val_loss: 0.4994 - val_accuracy: 0.8637 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "302/302 [==============================] - 196s 649ms/step - loss: 0.1071 - accuracy: 0.9626 - val_loss: 0.4081 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "302/302 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9730\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "302/302 [==============================] - 197s 653ms/step - loss: 0.0754 - accuracy: 0.9730 - val_loss: 0.4729 - val_accuracy: 0.8806 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "302/302 [==============================] - 197s 653ms/step - loss: 0.0359 - accuracy: 0.9889 - val_loss: 0.3994 - val_accuracy: 0.9113 - lr: 5.0000e-04\n",
      "Epoch 15/50\n",
      "302/302 [==============================] - 196s 650ms/step - loss: 0.0175 - accuracy: 0.9955 - val_loss: 0.4102 - val_accuracy: 0.9171 - lr: 5.0000e-04\n",
      "Epoch 16/50\n",
      "302/302 [==============================] - 198s 656ms/step - loss: 0.0136 - accuracy: 0.9961 - val_loss: 0.3940 - val_accuracy: 0.9204 - lr: 5.0000e-04\n",
      "Epoch 17/50\n",
      "302/302 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9956\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "302/302 [==============================] - 199s 658ms/step - loss: 0.0144 - accuracy: 0.9956 - val_loss: 0.5079 - val_accuracy: 0.9109 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_aug, y_train_aug,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fcdc362-b462-4728-9b38-e94845188c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 2s 23ms/step\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "   Abnormal Heartbeat       0.89      0.82      0.85       606\n",
      "        History of MI       0.80      0.79      0.79       447\n",
      "Myocardial Infarction       0.95      1.00      0.98       621\n",
      "               Normal       0.89      0.92      0.90       739\n",
      "\n",
      "             accuracy                           0.89      2413\n",
      "            macro avg       0.88      0.88      0.88      2413\n",
      "         weighted avg       0.89      0.89      0.89      2413\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[495  53  20  38]\n",
      " [ 42 351   5  49]\n",
      " [  0   0 621   0]\n",
      " [ 22  33   5 679]]\n",
      "\n",
      "Final Test Accuracy: 88.93%\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred_classes, target_names=le.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "# Final Accuracy\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nFinal Test Accuracy: {acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1952038-0acf-4823-be50-fb9965abba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "Improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a8c1ada-dfe1-4e3f-b135-59b0a4b2a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d51899b-ca8c-4009-9d9f-f537a8e04f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths\n",
    "folders = {\n",
    "    \"Abnormal Heartbeat\": r\"D:\\FYP\\Cadivas CNN\\preprocessed_1d\\AHB\",\n",
    "    \"Myocardial Infarction\": r\"D:\\FYP\\Cadivas CNN\\preprocessed_1d\\MI\",\n",
    "    \"Normal\": r\"D:\\FYP\\Cadivas CNN\\preprocessed_1d\\NORMAL\",\n",
    "    \"History of MI\": r\"D:\\FYP\\Cadivas CNN\\preprocessed_1d\\PM\"\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "for label, folder in folders.items():\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(folder, file))\n",
    "            df['Class'] = label\n",
    "            all_data.append(df)\n",
    "\n",
    "# Combine all data\n",
    "data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Features & Labels\n",
    "X = data.iloc[:, :255].values\n",
    "y = data['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84638f4f-f66b-43db-8c21-de85a3e0d02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (9651, 255, 1)  Test shape: (2413, 255, 1)\n"
     ]
    }
   ],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "# Save label encoder\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_onehot, test_size=0.2, random_state=42, stratify=y_onehot\n",
    ")\n",
    "\n",
    "# Reshape for 1D CNN\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \" Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67eca9bf-57c9-4899-9685-8a7a90a865b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_11 (Conv1D)          (None, 255, 128)          768       \n",
      "                                                                 \n",
      " max_pooling1d_11 (MaxPoolin  (None, 127, 128)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 127, 128)          0         \n",
      "                                                                 \n",
      " conv1d_12 (Conv1D)          (None, 127, 256)          164096    \n",
      "                                                                 \n",
      " max_pooling1d_12 (MaxPoolin  (None, 63, 256)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 63, 256)           0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 128)               197120    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 379,012\n",
      "Trainable params: 379,012\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], 1)),\n",
    "\n",
    "    Conv1D(128, kernel_size=5, activation='relu', padding='same'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Conv1D(256, kernel_size=5, activation='relu', padding='same'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    LSTM(128, return_sequences=False, dropout=0.2, recurrent_dropout=0.2),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(y_onehot.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8dac2ac9-3289-4e69-9547-39e54e19d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48135163-e39a-48f5-9ee6-2f39a7191959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "302/302 [==============================] - 40s 124ms/step - loss: 1.3718 - accuracy: 0.3016 - val_loss: 1.3598 - val_accuracy: 0.3079 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "302/302 [==============================] - 36s 120ms/step - loss: 1.3525 - accuracy: 0.3417 - val_loss: 1.3206 - val_accuracy: 0.3734 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "302/302 [==============================] - 38s 125ms/step - loss: 1.3084 - accuracy: 0.3882 - val_loss: 1.2579 - val_accuracy: 0.4057 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "302/302 [==============================] - 37s 122ms/step - loss: 1.2717 - accuracy: 0.4106 - val_loss: 1.2261 - val_accuracy: 0.4256 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "302/302 [==============================] - 37s 122ms/step - loss: 1.2260 - accuracy: 0.4489 - val_loss: 1.1713 - val_accuracy: 0.4737 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "302/302 [==============================] - 36s 121ms/step - loss: 1.1868 - accuracy: 0.4694 - val_loss: 1.1214 - val_accuracy: 0.4985 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "302/302 [==============================] - 38s 125ms/step - loss: 1.1294 - accuracy: 0.5046 - val_loss: 1.1021 - val_accuracy: 0.5222 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "302/302 [==============================] - 38s 126ms/step - loss: 1.0780 - accuracy: 0.5382 - val_loss: 0.9988 - val_accuracy: 0.5777 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "302/302 [==============================] - 38s 127ms/step - loss: 1.0389 - accuracy: 0.5588 - val_loss: 0.9596 - val_accuracy: 0.5893 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "302/302 [==============================] - 38s 127ms/step - loss: 0.9671 - accuracy: 0.5960 - val_loss: 0.8912 - val_accuracy: 0.6283 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "302/302 [==============================] - 39s 128ms/step - loss: 0.9123 - accuracy: 0.6255 - val_loss: 0.8627 - val_accuracy: 0.6502 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "302/302 [==============================] - 36s 120ms/step - loss: 0.8635 - accuracy: 0.6472 - val_loss: 0.7765 - val_accuracy: 0.6884 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "302/302 [==============================] - 36s 119ms/step - loss: 0.8320 - accuracy: 0.6597 - val_loss: 0.7875 - val_accuracy: 0.6730 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "302/302 [==============================] - 36s 118ms/step - loss: 0.7825 - accuracy: 0.6851 - val_loss: 0.7302 - val_accuracy: 0.7016 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "302/302 [==============================] - 36s 118ms/step - loss: 0.7335 - accuracy: 0.7028 - val_loss: 0.7035 - val_accuracy: 0.7140 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "302/302 [==============================] - 36s 118ms/step - loss: 0.7042 - accuracy: 0.7160 - val_loss: 0.6702 - val_accuracy: 0.7252 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "302/302 [==============================] - 36s 119ms/step - loss: 0.6788 - accuracy: 0.7317 - val_loss: 0.6717 - val_accuracy: 0.7232 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "302/302 [==============================] - 36s 119ms/step - loss: 0.6361 - accuracy: 0.7406 - val_loss: 0.5927 - val_accuracy: 0.7476 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "302/302 [==============================] - 36s 119ms/step - loss: 0.6148 - accuracy: 0.7475 - val_loss: 0.5724 - val_accuracy: 0.7741 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "302/302 [==============================] - 36s 119ms/step - loss: 0.5868 - accuracy: 0.7660 - val_loss: 0.5806 - val_accuracy: 0.7667 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "302/302 [==============================] - 36s 118ms/step - loss: 0.5658 - accuracy: 0.7705 - val_loss: 0.5304 - val_accuracy: 0.7866 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "302/302 [==============================] - 36s 118ms/step - loss: 0.5599 - accuracy: 0.7727 - val_loss: 0.5404 - val_accuracy: 0.7878 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "302/302 [==============================] - 36s 118ms/step - loss: 0.5210 - accuracy: 0.7891 - val_loss: 0.5242 - val_accuracy: 0.7824 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "302/302 [==============================] - 35s 118ms/step - loss: 0.5092 - accuracy: 0.7944 - val_loss: 0.5276 - val_accuracy: 0.7944 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "302/302 [==============================] - 36s 118ms/step - loss: 0.4841 - accuracy: 0.8057 - val_loss: 0.4927 - val_accuracy: 0.7994 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "302/302 [==============================] - 36s 118ms/step - loss: 0.4897 - accuracy: 0.8022 - val_loss: 0.4982 - val_accuracy: 0.8090 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "302/302 [==============================] - 36s 118ms/step - loss: 0.4776 - accuracy: 0.8048 - val_loss: 0.4818 - val_accuracy: 0.8085 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "302/302 [==============================] - 36s 118ms/step - loss: 0.4526 - accuracy: 0.8167 - val_loss: 0.5014 - val_accuracy: 0.8023 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "302/302 [==============================] - 35s 117ms/step - loss: 0.4423 - accuracy: 0.8213 - val_loss: 0.5045 - val_accuracy: 0.7982 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "302/302 [==============================] - 35s 117ms/step - loss: 0.4216 - accuracy: 0.8285 - val_loss: 0.4629 - val_accuracy: 0.8106 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "302/302 [==============================] - 35s 117ms/step - loss: 0.4222 - accuracy: 0.8314 - val_loss: 0.4294 - val_accuracy: 0.8259 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "302/302 [==============================] - 36s 118ms/step - loss: 0.4039 - accuracy: 0.8377 - val_loss: 0.4283 - val_accuracy: 0.8305 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "302/302 [==============================] - 35s 117ms/step - loss: 0.3830 - accuracy: 0.8448 - val_loss: 0.4310 - val_accuracy: 0.8346 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "302/302 [==============================] - 35s 117ms/step - loss: 0.3933 - accuracy: 0.8436 - val_loss: 0.4316 - val_accuracy: 0.8326 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "302/302 [==============================] - 35s 117ms/step - loss: 0.3720 - accuracy: 0.8516 - val_loss: 0.4407 - val_accuracy: 0.8309 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "302/302 [==============================] - 35s 117ms/step - loss: 0.3235 - accuracy: 0.8750 - val_loss: 0.3806 - val_accuracy: 0.8508 - lr: 5.0000e-04\n",
      "Epoch 37/50\n",
      "302/302 [==============================] - 35s 117ms/step - loss: 0.3021 - accuracy: 0.8804 - val_loss: 0.3773 - val_accuracy: 0.8529 - lr: 5.0000e-04\n",
      "Epoch 38/50\n",
      "302/302 [==============================] - 35s 117ms/step - loss: 0.2991 - accuracy: 0.8805 - val_loss: 0.3750 - val_accuracy: 0.8504 - lr: 5.0000e-04\n",
      "Epoch 39/50\n",
      "302/302 [==============================] - 35s 117ms/step - loss: 0.2874 - accuracy: 0.8835 - val_loss: 0.3682 - val_accuracy: 0.8645 - lr: 5.0000e-04\n",
      "Epoch 40/50\n",
      "302/302 [==============================] - 35s 117ms/step - loss: 0.2780 - accuracy: 0.8881 - val_loss: 0.3751 - val_accuracy: 0.8608 - lr: 5.0000e-04\n",
      "Epoch 41/50\n",
      "302/302 [==============================] - 35s 116ms/step - loss: 0.2856 - accuracy: 0.8862 - val_loss: 0.3892 - val_accuracy: 0.8599 - lr: 5.0000e-04\n",
      "Epoch 42/50\n",
      "302/302 [==============================] - 35s 117ms/step - loss: 0.2586 - accuracy: 0.8972 - val_loss: 0.3544 - val_accuracy: 0.8711 - lr: 5.0000e-04\n",
      "Epoch 43/50\n",
      "302/302 [==============================] - 35s 116ms/step - loss: 0.2722 - accuracy: 0.8923 - val_loss: 0.3582 - val_accuracy: 0.8666 - lr: 5.0000e-04\n",
      "Epoch 44/50\n",
      "302/302 [==============================] - 35s 117ms/step - loss: 0.2573 - accuracy: 0.9034 - val_loss: 0.3518 - val_accuracy: 0.8666 - lr: 5.0000e-04\n",
      "Epoch 45/50\n",
      "302/302 [==============================] - 35s 117ms/step - loss: 0.2483 - accuracy: 0.9019 - val_loss: 0.3424 - val_accuracy: 0.8699 - lr: 5.0000e-04\n",
      "Epoch 46/50\n",
      "302/302 [==============================] - 35s 117ms/step - loss: 0.2507 - accuracy: 0.9004 - val_loss: 0.3655 - val_accuracy: 0.8703 - lr: 5.0000e-04\n",
      "Epoch 47/50\n",
      "302/302 [==============================] - 35s 117ms/step - loss: 0.2355 - accuracy: 0.9074 - val_loss: 0.3504 - val_accuracy: 0.8699 - lr: 5.0000e-04\n",
      "Epoch 48/50\n",
      "302/302 [==============================] - 35s 116ms/step - loss: 0.2408 - accuracy: 0.9057 - val_loss: 0.3600 - val_accuracy: 0.8757 - lr: 5.0000e-04\n",
      "Epoch 49/50\n",
      "302/302 [==============================] - 35s 117ms/step - loss: 0.2174 - accuracy: 0.9147 - val_loss: 0.3245 - val_accuracy: 0.8902 - lr: 2.5000e-04\n",
      "Epoch 50/50\n",
      "302/302 [==============================] - 35s 117ms/step - loss: 0.2024 - accuracy: 0.9239 - val_loss: 0.3489 - val_accuracy: 0.8873 - lr: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1417c13a-6cc9-4604-82ad-e9e9dd8748b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 2s 32ms/step\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "   Abnormal Heartbeat       0.92      0.82      0.87       606\n",
      "        History of MI       0.79      0.71      0.75       447\n",
      "Myocardial Infarction       0.97      1.00      0.98       621\n",
      "               Normal       0.85      0.95      0.90       739\n",
      "\n",
      "             accuracy                           0.89      2413\n",
      "            macro avg       0.88      0.87      0.87      2413\n",
      "         weighted avg       0.89      0.89      0.89      2413\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[499  54  17  36]\n",
      " [ 41 318   2  86]\n",
      " [  0   0 621   0]\n",
      " [  4  29   3 703]]\n",
      "\n",
      "Final Test Accuracy: 88.73%\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\\n\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Final Accuracy\n",
    "acc = accuracy_score(y_true, y_pred) * 100\n",
    "print(f\"\\nFinal Test Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802669a0-06a5-452b-bc72-0ca1ec2e08f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Extra Conv1D + MaxPooling before LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1bb0cc2c-3f63-4a7a-af70-466020561a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08ccd825-de80-4e5f-b897-98ac9e4773f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths\n",
    "folders = {\n",
    "    \"Abnormal Heartbeat\": r\"D:\\FYP\\Cadivas CNN\\preprocessed_1d\\AHB\",\n",
    "    \"Myocardial Infarction\": r\"D:\\FYP\\Cadivas CNN\\preprocessed_1d\\MI\",\n",
    "    \"Normal\": r\"D:\\FYP\\Cadivas CNN\\preprocessed_1d\\NORMAL\",\n",
    "    \"History of MI\": r\"D:\\FYP\\Cadivas CNN\\preprocessed_1d\\PM\"\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "for label, folder in folders.items():\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(folder, file))\n",
    "            df['Class'] = label\n",
    "            all_data.append(df)\n",
    "\n",
    "data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Features & Labels\n",
    "X = data.iloc[:, :255].values\n",
    "y = data['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dfdca657-6e8a-4e57-ab76-2b837337ab34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (9651, 255, 1) Test shape: (2413, 255, 1)\n"
     ]
    }
   ],
   "source": [
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "# Save label encoder\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_onehot, test_size=0.2, random_state=42, stratify=y_onehot\n",
    ")\n",
    "\n",
    "# Reshape for 1D CNN\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a124337-545d-4366-b210-cabab91c85d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 0.9957697069748246, 1: 1.3486584684181107, 2: 0.9705349959774738, 3: 0.8170504571622079}\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(np.argmax(y_train, axis=1)),\n",
    "    y=np.argmax(y_train, axis=1)\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fe959e9f-914a-4991-b579-301f4e26b65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_13 (Conv1D)          (None, 251, 64)           384       \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 251, 64)          256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_13 (MaxPoolin  (None, 125, 64)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 125, 64)           0         \n",
      "                                                                 \n",
      " conv1d_14 (Conv1D)          (None, 121, 128)          41088     \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 121, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d_14 (MaxPoolin  (None, 60, 128)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 60, 128)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182,340\n",
      "Trainable params: 181,956\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Conv1D layers\n",
    "model.add(Conv1D(64, kernel_size=5, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(128, kernel_size=5, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# LSTM layer\n",
    "model.add(LSTM(128, return_sequences=False, dropout=0.2))\n",
    "\n",
    "# Fully connected\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "48a958ca-4211-45a0-8777-7a1581c805f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eeafe3e4-f543-4998-9102-bbb42185fc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "302/302 [==============================] - 35s 105ms/step - loss: 1.3587 - accuracy: 0.3318 - val_loss: 1.5823 - val_accuracy: 0.1852 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "302/302 [==============================] - 31s 103ms/step - loss: 1.2717 - accuracy: 0.4145 - val_loss: 1.5242 - val_accuracy: 0.3270 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "302/302 [==============================] - 31s 102ms/step - loss: 1.2045 - accuracy: 0.4652 - val_loss: 1.2036 - val_accuracy: 0.4579 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "302/302 [==============================] - 31s 103ms/step - loss: 1.1252 - accuracy: 0.5130 - val_loss: 1.1811 - val_accuracy: 0.5077 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "302/302 [==============================] - 31s 102ms/step - loss: 1.0416 - accuracy: 0.5632 - val_loss: 1.0248 - val_accuracy: 0.5636 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "302/302 [==============================] - 32s 105ms/step - loss: 0.9601 - accuracy: 0.6024 - val_loss: 1.0836 - val_accuracy: 0.5466 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "302/302 [==============================] - 31s 104ms/step - loss: 0.8921 - accuracy: 0.6408 - val_loss: 0.9053 - val_accuracy: 0.6262 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "302/302 [==============================] - 31s 104ms/step - loss: 0.8262 - accuracy: 0.6666 - val_loss: 1.0130 - val_accuracy: 0.5794 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "302/302 [==============================] - 31s 103ms/step - loss: 0.7724 - accuracy: 0.6945 - val_loss: 0.7020 - val_accuracy: 0.7140 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "302/302 [==============================] - 32s 105ms/step - loss: 0.7342 - accuracy: 0.7074 - val_loss: 0.6520 - val_accuracy: 0.7476 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "302/302 [==============================] - 33s 108ms/step - loss: 0.6726 - accuracy: 0.7347 - val_loss: 0.6868 - val_accuracy: 0.7165 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "302/302 [==============================] - 32s 106ms/step - loss: 0.6461 - accuracy: 0.7366 - val_loss: 0.6530 - val_accuracy: 0.7373 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "302/302 [==============================] - 32s 107ms/step - loss: 0.6151 - accuracy: 0.7593 - val_loss: 0.5765 - val_accuracy: 0.7563 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "302/302 [==============================] - 33s 108ms/step - loss: 0.5752 - accuracy: 0.7737 - val_loss: 0.6300 - val_accuracy: 0.7414 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "302/302 [==============================] - 33s 108ms/step - loss: 0.5504 - accuracy: 0.7832 - val_loss: 0.6397 - val_accuracy: 0.7480 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "302/302 [==============================] - 33s 108ms/step - loss: 0.5301 - accuracy: 0.7900 - val_loss: 0.6072 - val_accuracy: 0.7596 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "302/302 [==============================] - 33s 108ms/step - loss: 0.4966 - accuracy: 0.8054 - val_loss: 0.5018 - val_accuracy: 0.8019 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "302/302 [==============================] - 33s 108ms/step - loss: 0.4873 - accuracy: 0.8060 - val_loss: 0.5342 - val_accuracy: 0.8002 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "302/302 [==============================] - 32s 106ms/step - loss: 0.4625 - accuracy: 0.8211 - val_loss: 0.4820 - val_accuracy: 0.8143 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "302/302 [==============================] - 33s 108ms/step - loss: 0.4566 - accuracy: 0.8219 - val_loss: 0.7243 - val_accuracy: 0.7219 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "302/302 [==============================] - 32s 107ms/step - loss: 0.4363 - accuracy: 0.8336 - val_loss: 0.5209 - val_accuracy: 0.7982 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "302/302 [==============================] - 31s 103ms/step - loss: 0.4168 - accuracy: 0.8403 - val_loss: 0.4486 - val_accuracy: 0.8322 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "302/302 [==============================] - 32s 107ms/step - loss: 0.3940 - accuracy: 0.8501 - val_loss: 0.4389 - val_accuracy: 0.8355 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "302/302 [==============================] - 33s 108ms/step - loss: 0.3787 - accuracy: 0.8552 - val_loss: 0.5984 - val_accuracy: 0.7741 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "302/302 [==============================] - 32s 107ms/step - loss: 0.3514 - accuracy: 0.8703 - val_loss: 0.5471 - val_accuracy: 0.8090 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "302/302 [==============================] - 32s 107ms/step - loss: 0.3390 - accuracy: 0.8714 - val_loss: 0.4094 - val_accuracy: 0.8562 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "302/302 [==============================] - 32s 107ms/step - loss: 0.3405 - accuracy: 0.8715 - val_loss: 0.5331 - val_accuracy: 0.8094 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "302/302 [==============================] - 33s 108ms/step - loss: 0.3200 - accuracy: 0.8810 - val_loss: 0.5792 - val_accuracy: 0.7928 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "302/302 [==============================] - 32s 108ms/step - loss: 0.3270 - accuracy: 0.8767 - val_loss: 0.5140 - val_accuracy: 0.8218 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "302/302 [==============================] - 32s 107ms/step - loss: 0.3093 - accuracy: 0.8815 - val_loss: 0.4085 - val_accuracy: 0.8545 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "302/302 [==============================] - 32s 107ms/step - loss: 0.2869 - accuracy: 0.8950 - val_loss: 0.5274 - val_accuracy: 0.8201 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "302/302 [==============================] - 32s 107ms/step - loss: 0.2817 - accuracy: 0.8977 - val_loss: 0.3764 - val_accuracy: 0.8695 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "302/302 [==============================] - 33s 108ms/step - loss: 0.2679 - accuracy: 0.9023 - val_loss: 0.4131 - val_accuracy: 0.8579 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "302/302 [==============================] - 33s 109ms/step - loss: 0.2659 - accuracy: 0.9020 - val_loss: 0.5914 - val_accuracy: 0.8019 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "302/302 [==============================] - 33s 108ms/step - loss: 0.2490 - accuracy: 0.9080 - val_loss: 0.6100 - val_accuracy: 0.8139 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "302/302 [==============================] - 32s 107ms/step - loss: 0.2595 - accuracy: 0.9074 - val_loss: 0.4568 - val_accuracy: 0.8566 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "302/302 [==============================] - 32s 107ms/step - loss: 0.1879 - accuracy: 0.9319 - val_loss: 0.3399 - val_accuracy: 0.8827 - lr: 5.0000e-04\n",
      "Epoch 38/50\n",
      "302/302 [==============================] - 32s 106ms/step - loss: 0.1773 - accuracy: 0.9344 - val_loss: 0.3924 - val_accuracy: 0.8840 - lr: 5.0000e-04\n",
      "Epoch 39/50\n",
      "302/302 [==============================] - 32s 106ms/step - loss: 0.1758 - accuracy: 0.9374 - val_loss: 0.3754 - val_accuracy: 0.8906 - lr: 5.0000e-04\n",
      "Epoch 40/50\n",
      "302/302 [==============================] - 32s 107ms/step - loss: 0.1562 - accuracy: 0.9422 - val_loss: 0.3898 - val_accuracy: 0.8819 - lr: 5.0000e-04\n",
      "Epoch 41/50\n",
      "302/302 [==============================] - 33s 109ms/step - loss: 0.1575 - accuracy: 0.9419 - val_loss: 0.4158 - val_accuracy: 0.8728 - lr: 5.0000e-04\n",
      "Epoch 42/50\n",
      "302/302 [==============================] - 32s 107ms/step - loss: 0.1313 - accuracy: 0.9526 - val_loss: 0.3560 - val_accuracy: 0.8972 - lr: 2.5000e-04\n",
      "Epoch 43/50\n",
      "302/302 [==============================] - 33s 108ms/step - loss: 0.1309 - accuracy: 0.9550 - val_loss: 0.3594 - val_accuracy: 0.9014 - lr: 2.5000e-04\n",
      "Epoch 44/50\n",
      "302/302 [==============================] - 32s 107ms/step - loss: 0.1228 - accuracy: 0.9551 - val_loss: 0.3695 - val_accuracy: 0.8997 - lr: 2.5000e-04\n",
      "Epoch 45/50\n",
      "302/302 [==============================] - 33s 108ms/step - loss: 0.1185 - accuracy: 0.9583 - val_loss: 0.3608 - val_accuracy: 0.8997 - lr: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0e427a93-66e4-4c19-bdcb-0e5492fee198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 4s 42ms/step\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "   Abnormal Heartbeat       0.93      0.80      0.86       606\n",
      "        History of MI       0.74      0.84      0.78       447\n",
      "Myocardial Infarction       0.95      1.00      0.97       621\n",
      "               Normal       0.89      0.88      0.89       739\n",
      "\n",
      "             accuracy                           0.88      2413\n",
      "            macro avg       0.88      0.88      0.88      2413\n",
      "         weighted avg       0.89      0.88      0.88      2413\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "[[485  68  20  33]\n",
      " [ 25 375   2  45]\n",
      " [  0   0 621   0]\n",
      " [ 12  67  11 649]]\n",
      "\n",
      "Final Test Accuracy: 88.27%\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\\n\")\n",
    "print(confusion_matrix(y_true, y_pred_classes))\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred_classes) * 100\n",
    "print(f\"\\nFinal Test Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5267664-9732-43d5-926a-72b8acf83a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN + Bidirectional LSTM + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3aaa153b-788e-4daf-a405-8aa64970ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (Input, Conv1D, MaxPooling1D, BatchNormalization,\n",
    "                                     Dropout, Bidirectional, LSTM, Dense, Flatten, Layer)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# reproducibility (best-effort)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "49c784d8-02fe-42af-851c-b24bf589c941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded samples: 12064\n"
     ]
    }
   ],
   "source": [
    "# Cell 2\n",
    "folders = {\n",
    "    \"Abnormal Heartbeat\": r\"D:\\FYP\\Cadivas CNN\\preprocessed_1d\\AHB\",\n",
    "    \"Myocardial Infarction\": r\"D:\\FYP\\Cadivas CNN\\preprocessed_1d\\MI\",\n",
    "    \"Normal\": r\"D:\\FYP\\Cadivas CNN\\preprocessed_1d\\NORMAL\",\n",
    "    \"History of MI\": r\"D:\\FYP\\Cadivas CNN\\preprocessed_1d\\PM\"\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "for label, folder in folders.items():\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(folder, file))\n",
    "            df['Class'] = label\n",
    "            all_data.append(df)\n",
    "\n",
    "data = pd.concat(all_data, ignore_index=True)\n",
    "print(\"Loaded samples:\", data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b4d6a463-26d0-4e4b-b354-4b18923159f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3\n",
    "# features: first 255 columns (as before)\n",
    "X = data.iloc[:, :255].values.astype(np.float32)\n",
    "y = data['Class'].values\n",
    "\n",
    "# Standardize features (fit on whole data or only train; we will fit on whole here for simplicity)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Save scaler if you want later (optional)\n",
    "joblib.dump(scaler, \"scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8a0a2b69-f51a-42b9-9f2b-568b4073b961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Abnormal Heartbeat' 'History of MI' 'Myocardial Infarction' 'Normal']\n",
      "Shapes: (9651, 255, 1) (2413, 255, 1) (9651, 4)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "# Save encoder\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "print(\"Classes:\", label_encoder.classes_)\n",
    "\n",
    "# Train/test split (stratify by original labels)\n",
    "X_train, X_test, y_train, y_test, y_train_labels, y_test_labels = train_test_split(\n",
    "    X, y_onehot, y_encoded, test_size=0.2, random_state=SEED, stratify=y_onehot\n",
    ")\n",
    "\n",
    "# reshape to (samples, timesteps, channels)\n",
    "X_train = X_train.reshape((-1, X_train.shape[1], 1))\n",
    "X_test  = X_test.reshape((-1, X_test.shape[1], 1))\n",
    "\n",
    "print(\"Shapes:\", X_train.shape, X_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "600f53ae-c864-49a9-81d4-a35d030da0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented train shape: (28953, 255, 1) (28953, 4)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5\n",
    "def add_gaussian_noise(X, noise_factor=0.01):\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_factor, size=X.shape)\n",
    "    return X + noise\n",
    "\n",
    "def time_shift(X, max_shift=5):\n",
    "    # shift along timestep axis by up to max_shift\n",
    "    Xs = []\n",
    "    for sample in X:\n",
    "        shift = np.random.randint(-max_shift, max_shift+1)\n",
    "        if shift == 0:\n",
    "            Xs.append(sample)\n",
    "        elif shift > 0:\n",
    "            shifted = np.vstack([np.zeros((shift,1)), sample[:-shift]])\n",
    "            Xs.append(shifted)\n",
    "        else:\n",
    "            shifted = np.vstack([sample[-shift:], np.zeros((-shift,1))])\n",
    "            Xs.append(shifted)\n",
    "    return np.array(Xs)\n",
    "\n",
    "# create augmented training set by mixing original + noisy + shifted\n",
    "X_train_noisy = add_gaussian_noise(X_train, noise_factor=0.02)\n",
    "X_train_shift = time_shift(X_train, max_shift=6)\n",
    "\n",
    "X_train_aug = np.concatenate([X_train, X_train_noisy, X_train_shift], axis=0)\n",
    "y_train_aug = np.concatenate([y_train, y_train, y_train], axis=0)\n",
    "\n",
    "print(\"Augmented train shape:\", X_train_aug.shape, y_train_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1371049b-c9db-48ab-b50b-87cb6a6ecbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.9957697069748246, 1: 1.3486584684181107, 2: 0.9705349959774738, 3: 0.8170504571622079}\n"
     ]
    }
   ],
   "source": [
    "orig_train_labels = np.argmax(y_train, axis=1)  # or use y_train_labels from earlier split\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(orig_train_labels),\n",
    "    y=orig_train_labels\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights_array))\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44141840-57d5-4917-bdcc-a9d45cb907a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "# Simple attention layer: computes attention over time axis\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape: (batch, time_steps, features)\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1],), initializer='random_normal', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch, time, features)\n",
    "        # score: (batch, time)\n",
    "        score = tf.tensordot(inputs, self.W, axes=[2, 0])  # dot over features -> (batch, time)\n",
    "        weights = tf.nn.softmax(score, axis=1)             # (batch, time)\n",
    "        weights_expanded = tf.expand_dims(weights, axis=-1) # (batch, time, 1)\n",
    "        context = tf.reduce_sum(inputs * weights_expanded, axis=1) # (batch, features)\n",
    "        return context\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(AttentionLayer, self).get_config()\n",
    "        return {**base_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6f9228eb-69b6-415f-bebe-d0ef05fecbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 255, 1)]          0         \n",
      "                                                                 \n",
      " conv1d_15 (Conv1D)          (None, 255, 128)          768       \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 255, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d_15 (MaxPoolin  (None, 127, 128)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 127, 128)          0         \n",
      "                                                                 \n",
      " conv1d_16 (Conv1D)          (None, 127, 256)          98560     \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 127, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d_16 (MaxPoolin  (None, 63, 256)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 63, 256)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 63, 256)          394240    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " attention_layer (AttentionL  (None, 256)              256       \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 528,772\n",
      "Trainable params: 528,004\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Cell 8\n",
    "timesteps = X_train.shape[1]\n",
    "channels = X_train.shape[2]\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "inputs = Input(shape=(timesteps, channels))\n",
    "\n",
    "# Conv blocks for feature extraction\n",
    "x = Conv1D(128, kernel_size=5, padding='same', activation='relu')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv1D(256, kernel_size=3, padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# Bidirectional LSTM to capture temporal patterns\n",
    "x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.1))(x)\n",
    "\n",
    "# Attention mechanism to aggregate time steps\n",
    "context = AttentionLayer()(x)\n",
    "\n",
    "# Dense head\n",
    "x = Dense(128, activation='relu')(context)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6ef877-9804-468d-b73a-b6a60f311675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "453/453 [==============================] - 522s 1s/step - loss: 1.1309 - accuracy: 0.5037 - val_loss: 1.3916 - val_accuracy: 0.4194 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "453/453 [==============================] - 547s 1s/step - loss: 0.8672 - accuracy: 0.6474 - val_loss: 0.8721 - val_accuracy: 0.6328 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "453/453 [==============================] - 591s 1s/step - loss: 0.7070 - accuracy: 0.7200 - val_loss: 0.6869 - val_accuracy: 0.7219 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "453/453 [==============================] - 613s 1s/step - loss: 0.5968 - accuracy: 0.7625 - val_loss: 0.6281 - val_accuracy: 0.7455 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "453/453 [==============================] - 647s 1s/step - loss: 0.5139 - accuracy: 0.7964 - val_loss: 0.5626 - val_accuracy: 0.7895 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "453/453 [==============================] - 679s 1s/step - loss: 0.4406 - accuracy: 0.8298 - val_loss: 0.5596 - val_accuracy: 0.8007 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "453/453 [==============================] - 641s 1s/step - loss: 0.3847 - accuracy: 0.8515 - val_loss: 0.4824 - val_accuracy: 0.8222 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "453/453 [==============================] - 644s 1s/step - loss: 0.3386 - accuracy: 0.8697 - val_loss: 0.4606 - val_accuracy: 0.8355 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "384/453 [========================>.....] - ETA: 1:37 - loss: 0.3108 - accuracy: 0.8809"
     ]
    }
   ],
   "source": [
    "# Cell 9\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_aug, y_train_aug,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b28c8-47d7-43e0-85a2-e85419dde414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "# plot accuracy & loss\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='train_acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save history if desired\n",
    "import json\n",
    "with open(\"training_history.json\", \"w\") as f:\n",
    "    json.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8da2a41-7812-4228-8603-761c0cfa4962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(cm, interpolation='nearest', aspect='auto')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(label_encoder.classes_))\n",
    "plt.xticks(tick_marks, label_encoder.classes_, rotation=45, ha='right')\n",
    "plt.yticks(tick_marks, label_encoder.classes_)\n",
    "# annotate cells\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred) * 100\n",
    "print(f\"\\nFinal Test Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c00c2-5e90-4163-b2d7-85fbb1b08e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import joblib\n",
    "\n",
    "# -----------------------------\n",
    "# Load & Combine Data\n",
    "# -----------------------------\n",
    "folders = {\n",
    "    \"Abnormal Heartbeat\": r\"E:\\FYP\\Cardi 2\\Cardiovascular-Detection-using-ECG-images\\preprocessed_1d\\AHB\",\n",
    "    \"Myocardial Infarction\": r\"E:\\FYP\\Cardi 2\\Cardiovascular-Detection-using-ECG-images\\preprocessed_1d\\MI\",\n",
    "    \"Normal\": r\"E:\\FYP\\Cardi 2\\Cardiovascular-Detection-using-ECG-images\\preprocessed_1d\\NORMAL\",\n",
    "    \"History of MI\": r\"E:\\FYP\\Cardi 2\\Cardiovascular-Detection-using-ECG-images\\preprocessed_1d\\PM\"\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "for label, folder in folders.items():\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(folder, file))\n",
    "            df['Class'] = label\n",
    "            all_data.append(df)\n",
    "\n",
    "data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Save combined CSV\n",
    "save_path = r\"E:\\FYP\\Cardi 2\\Cardiovascular-Detection-using-ECG-images\\combined_data.csv\"\n",
    "data.to_csv(save_path, index=False)\n",
    "print(f\"Combined CSV saved at: {save_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare Data\n",
    "# -----------------------------\n",
    "X = data.iloc[:, :255].values\n",
    "y = data['Class'].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_onehot, test_size=0.2, random_state=42, stratify=y_onehot\n",
    ")\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# -----------------------------\n",
    "# Improved CNN Model\n",
    "# -----------------------------\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], 1)),\n",
    "\n",
    "    Conv1D(64, kernel_size=7, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Conv1D(128, kernel_size=5, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Conv1D(256, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(y_onehot.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# -----------------------------\n",
    "# Callbacks (better training)\n",
    "# -----------------------------\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "# -----------------------------\n",
    "# Train Model\n",
    "# -----------------------------\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate\n",
    "# -----------------------------\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\" Test Accuracy: {acc*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
